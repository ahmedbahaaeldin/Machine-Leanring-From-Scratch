{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks From Scratch\n",
    "   In the following Notebook , I will create a neural network from scratch where every process is splitted into a function.\n",
    "   The processes are :<br>\n",
    "   1- Initializing the weights and biases according to the dimensions of Input data and the required number of hidden layers.<br>\n",
    "   2- Forward propagation , where we propagate our Input through the hidden layers to calculate the Output.<br>\n",
    "   3- Backward propagation , where we propagate our Error or Loss backward through the Layers to be able to calculate the      dLoss/dWeights which means how to minimize the loss by changing these weights.<br>\n",
    "   4- Updating these Weights.<br>\n",
    "   5- Then Iterate to step 2.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import our needed libraries \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    #download_mnist()\n",
    "    #save_mnist()\n",
    "    load()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaded the dataset into the following X's and Y's with numpy format\n",
    "X_train,Y_train,X_test,Y_test=load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Inputs to be between 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = Y_train\n",
    "maxx = np.max(values)+1\n",
    "Y_trainz= np.eye(maxx)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Y_test\n",
    "asd = np.max(v)+1\n",
    "Y_testz=np.eye(asd)[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now define the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(X):\n",
    "    return np.tanh(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_Entropy_Loss(Output,Predicted):\n",
    "    m = Output.shape[0]\n",
    "    loss = (-1/m )* np.sum(Output*np.log(Predicted.clip(min=0.00000001)))\n",
    "    return loss\n",
    "\n",
    "def Loss_derivative(Output,Predicted):\n",
    "    return (Predicted - Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(X):\n",
    "    return (1 - np.power(X,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialize_Parameters(Input_dim,Hidden_dim,Output_dim):\n",
    "    W1 = np.random.randn(Input_dim,Hidden_dim)\n",
    "    b1 = np.zeros((1,Hidden_dim))\n",
    "    \n",
    "    W2 = np.random.randn(Hidden_dim,Hidden_dim)\n",
    "    b2 = np.zeros((1,Hidden_dim))\n",
    "    \n",
    "    W3 = np.random.randn(Hidden_dim,Output_dim)\n",
    "    b3 = np.zeros((1,Output_dim))\n",
    "    \n",
    "    Parameters = {'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3}\n",
    "    return Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_Prop(Parameters,x):\n",
    "    W1,b1,W2,b2,W3,b3= Parameters['W1'],Parameters['b1'],Parameters['W2'],Parameters['b2'],Parameters['W3'],Parameters['b3']\n",
    "    \n",
    "    Z1 = np.dot(x,W1) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = np.dot(A1,W2) + b2\n",
    "    A2 = tanh(Z2)\n",
    "    Z3 = np.dot(A2,W3) + b3  \n",
    "    A3 = Softmax(Z3)\n",
    "    \n",
    "    Layers = {'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2,'Z3':Z3,'A3':A3}\n",
    "    return Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_prop(Parameters,Layers,x,y):\n",
    "    Z1,A1,Z2,A2,Z3,A3 = Layers['Z1'],Layers['A1'],Layers['Z2'],Layers['A2'],Layers['Z3'],Layers['A3']\n",
    "    W2,W3 = Parameters['W2'],Parameters['W3']\n",
    "    m = x.shape[0]\n",
    "    dZ3 = Loss_derivative(y,A3)\n",
    "    dW3 = (1/m) * np.dot((A2.T),dZ3)\n",
    "    db3 = (1/m) * np.sum(dZ3,axis=0)\n",
    "    dZ2 = np.multiply(dZ3.dot(W3.T),tanh_derivative(A2))\n",
    "    dW2 = (1/m) * np.dot((A1.T),dZ2)\n",
    "    db2 = (1/m) * np.sum(dZ2,axis=0)\n",
    "    dZ1 = np.multiply(dZ2.dot(W2.T),tanh_derivative(A1))\n",
    "    dW1 = (1/m) * np.dot((x.T),dZ1)\n",
    "    db1 = (1/m) * np.sum(dZ1,axis=0)\n",
    "    \n",
    "    derivatives = {'db3':db3,'dW3':dW3,'db2':db2,'dW2':dW2,'db1':db1,'dW1':dW1}\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Process and Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Learning_Process(derivatives,Parameters,learning_rate):\n",
    "    dW3,db3,dW2,db2,dW1,db1 = derivatives['dW3'],derivatives['db3'],derivatives['dW2'],derivatives['db2'],derivatives['dW1'],derivatives['db1']\n",
    "    W3,b3,W2,b2,W1,b1 = Parameters['W3'],Parameters['b3'],Parameters['W2'],Parameters['b2'],Parameters['W1'],Parameters['b1']\n",
    "    \n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    W3 = W3 - learning_rate*dW3\n",
    "    \n",
    "    b1 = b1 - learning_rate*db1\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    b3 = b3 - learning_rate*db3\n",
    "    \n",
    "    Parameters = {'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3}\n",
    "    return Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_Loop(Parameters,learning_rate,X,Y,epochs):\n",
    "    for epoch in range(1,epochs):\n",
    "        Layers = forward_Prop(Parameters,X)\n",
    "        derivatives = Backward_prop(Parameters,Layers,X,Y)\n",
    "        Parameters = Learning_Process(derivatives,Parameters,learning_rate)\n",
    "        Loss = Cross_Entropy_Loss(Y,Layers['A3'])\n",
    "        if (epoch% 100==0):\n",
    "            print(\"Epoch %d : Loss is equal %f\"%(epoch,Loss))\n",
    "            print(accuracy_score(y_pred = np.argmax(Layers['A3'],axis=1),y_true = np.argmax(Y,axis=1)))\n",
    "    return Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 : Loss is equal 3.466147\n",
      "0.5511666666666667\n",
      "Epoch 200 : Loss is equal 2.378573\n",
      "0.6537333333333334\n",
      "Epoch 300 : Loss is equal 1.907731\n",
      "0.7021\n",
      "Epoch 400 : Loss is equal 1.626646\n",
      "0.7338166666666667\n",
      "Epoch 500 : Loss is equal 1.432608\n",
      "0.7540166666666667\n",
      "Epoch 600 : Loss is equal 1.287136\n",
      "0.7698333333333334\n",
      "Epoch 700 : Loss is equal 1.173637\n",
      "0.7832833333333333\n",
      "Epoch 800 : Loss is equal 1.082623\n",
      "0.7948166666666666\n",
      "Epoch 900 : Loss is equal 1.007562\n",
      "0.8051\n",
      "Epoch 1000 : Loss is equal 0.943995\n",
      "0.8135\n",
      "Epoch 1100 : Loss is equal 0.889069\n",
      "0.8205\n",
      "Epoch 1200 : Loss is equal 0.840753\n",
      "0.8263666666666667\n",
      "Epoch 1300 : Loss is equal 0.797892\n",
      "0.83155\n",
      "Epoch 1400 : Loss is equal 0.759705\n",
      "0.83635\n",
      "Epoch 1500 : Loss is equal 0.725446\n",
      "0.8413166666666667\n",
      "Epoch 1600 : Loss is equal 0.694516\n",
      "0.8456166666666667\n",
      "Epoch 1700 : Loss is equal 0.666582\n",
      "0.8499\n",
      "Epoch 1800 : Loss is equal 0.641336\n",
      "0.8537166666666667\n",
      "Epoch 1900 : Loss is equal 0.618373\n",
      "0.8572333333333333\n"
     ]
    }
   ],
   "source": [
    "Parameters=Initialize_Parameters(784,128,10)\n",
    "Parameters=Training_Loop(Parameters=Parameters,learning_rate=0.1,X=X_train,Y=Y_trainz,epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.61"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict = forward_Prop(Parameters,X_test)\n",
    "Predicted_labels = np.argmax(Predict['A3'],axis=1)\n",
    "accuracy_score(y_pred=Predicted_labels,y_true=Y_test)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
